{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home This site contains notes and resources about machine learning, data science and big data related tools and technologies. Resources Microsoft [Amazon] [Coursera]","title":"Home"},{"location":"#home","text":"This site contains notes and resources about machine learning, data science and big data related tools and technologies.","title":"Home"},{"location":"#resources","text":"Microsoft [Amazon] [Coursera]","title":"Resources"},{"location":"data-science/","text":"Data Science Data science revolves around data. Data can be traditional small amount of data and big data. First we need to pre-process it into a usable form For traditional data class labelling is done (categorical vs numerical) data cleaning is done missing valued are dealt with case specific steps are also done, like balancing and shuffling datasets (like changing the schema of database, etc) e.g. - basic customer data, historical stock price data Tools: Programming: R, Python, SQL, Matlab Software: Excel, IBM SPSS Who does this: DATA ARCHITECT DATA ENGINEER DATABASE ADMINISTRATOR For big data preprocessing is done same as with traditional data case specific steps are also done, like text data mining e.g. - social media, financial trade data Tools: Programming: R, Python, Java, Scala Software: hadoop, HBASE, mongoDB Who does this: BIG DATA ARCHITECT BIG DATA ENGINEER Now the data is processed in usable form, we use data to create reports and dashboards to gain business insights What is done? data is analyzed, and info is extracted from it in form of: KPI metrics reports dashboards e.g. - price optimization, inventory management Tools: Programming: R, Python, SQL, Matlab Tools: Excel, PowerBI, SAS, Qlik, tableau Who does this: BI ANALYST BI CONSULTANT BI DEVELOPER Now we do Predictive Analysis on the data. We can either use traditional techniques or machine learning Using traditional techniques advance statistical methods What is used: Regression Logistic Regression Clustering Factor Analysis Time Series e.g - User Experience, Sales Forecasting Tools: Programming: R, Python, Matlab Software: Excel, IBM SPSS, EViews, STATA Who does this DATA SCIENTIST DATA ANALYST Using Machine Learning What is used: Supervised Learning SVM NN deep learning random forests bayesian networks Unsupervised Learning K-means deep learning Reinforcement learning e.g. fraud detection, client retention Tools: Programming: R, Python, Matlab, java, js, c, scala, c++ Software: Microsoft Azure, rapidminer Who does this: DATA SCIENTIST MACHINE LEARNING ENGINEER Resources https://github.com/jakevdp/PythonDataScienceHandbook","title":"Data Science"},{"location":"data-science/#data-science","text":"Data science revolves around data. Data can be traditional small amount of data and big data.","title":"Data Science"},{"location":"data-science/#first-we-need-to-pre-process-it-into-a-usable-form","text":"For traditional data class labelling is done (categorical vs numerical) data cleaning is done missing valued are dealt with case specific steps are also done, like balancing and shuffling datasets (like changing the schema of database, etc) e.g. - basic customer data, historical stock price data Tools: Programming: R, Python, SQL, Matlab Software: Excel, IBM SPSS Who does this: DATA ARCHITECT DATA ENGINEER DATABASE ADMINISTRATOR For big data preprocessing is done same as with traditional data case specific steps are also done, like text data mining e.g. - social media, financial trade data Tools: Programming: R, Python, Java, Scala Software: hadoop, HBASE, mongoDB Who does this: BIG DATA ARCHITECT BIG DATA ENGINEER","title":"First we need to pre-process it into a usable form"},{"location":"data-science/#now-the-data-is-processed-in-usable-form-we-use-data-to-create-reports-and-dashboards-to-gain-business-insights","text":"What is done? data is analyzed, and info is extracted from it in form of: KPI metrics reports dashboards e.g. - price optimization, inventory management Tools: Programming: R, Python, SQL, Matlab Tools: Excel, PowerBI, SAS, Qlik, tableau Who does this: BI ANALYST BI CONSULTANT BI DEVELOPER","title":"Now the data is processed in usable form, we use data to create reports and dashboards to gain business insights"},{"location":"data-science/#now-we-do-predictive-analysis-on-the-data-we-can-either-use-traditional-techniques-or-machine-learning","text":"Using traditional techniques advance statistical methods What is used: Regression Logistic Regression Clustering Factor Analysis Time Series e.g - User Experience, Sales Forecasting Tools: Programming: R, Python, Matlab Software: Excel, IBM SPSS, EViews, STATA Who does this DATA SCIENTIST DATA ANALYST Using Machine Learning What is used: Supervised Learning SVM NN deep learning random forests bayesian networks Unsupervised Learning K-means deep learning Reinforcement learning e.g. fraud detection, client retention Tools: Programming: R, Python, Matlab, java, js, c, scala, c++ Software: Microsoft Azure, rapidminer Who does this: DATA SCIENTIST MACHINE LEARNING ENGINEER","title":"Now we do Predictive Analysis on the data. We can either use traditional techniques or machine learning"},{"location":"data-science/#resources","text":"https://github.com/jakevdp/PythonDataScienceHandbook","title":"Resources"},{"location":"hadoop/","text":"Hadoop BigData Big data is a collection of large datasets that cannot be processed using traditional computing techniques. It is not a single technique or a tool, rather it has become a complete subject, which involves various tools, techniques and frameworks. Structured data - Relational data. Semi Structured data - XML data. Unstructured data - Word, PDF, Text, Media Logs. Properties Volume huge volume of data in terabytes Velocity Variety Veracity Value Hadoop Apache Hadoop is a collection of open-source software utilities that facilitates using a network of many computers to solve problems involving massive amounts of data and computation. At its core, Hadoop has two major layers namely Processing/Computation layer (MapReduce), and Storage layer (Hadoop Distributed File System). Ecosystem Hadoop Common contains libraries and utilities needed by the other hadoop modules Hadoop Distributed file system (HDFS) distributed file system that stores data on commodity hardware machines, providing very high aggregates bandwidth across the cluster Hadoop YARN platform responsible for managing computing resources in clusters and using them for scheduling users applications Hadoop MapReduce an implementation of the MapReduce programming model for large scale data processing Hadoop Ozone an object store for hadoop Apache Cassandra NoSQL DBMS constructed to manage huge volumes of data spread across numerous commodity servers, delivering high availability. MapReduce MapReduce is a parallel programming model for writing distributed applications devised at Google for efficient processing of large amounts of data (multi-terabyte data-sets), on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner. The MapReduce program runs on Hadoop which is an Apache open-source framework. HDFS - Hadoop Distributed File System NameNode master daemon maintains and manages datanode records metadata, e.g. location of block stored, the size of the files, permissions, hierarchy, etc. receives heartbeat and block report form all the DataNode DataNode Slave deamons stores actual data serves read and write requests The Hadoop Distributed File System (HDFS) is based on the Google File System (GFS) and provides a distributed file system that is designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. It is highly fault-tolerant and is designed to be deployed on low-cost hardware. It provides high throughput access to application data and is suitable for applications having large datasets. Hadoop Common These are Java libraries and utilities required by other Hadoop modules. Hadoop YARN This is a framework for job scheduling and cluster resource management. resource manager node manager","title":"Hadoop"},{"location":"hadoop/#hadoop","text":"","title":"Hadoop"},{"location":"hadoop/#bigdata","text":"Big data is a collection of large datasets that cannot be processed using traditional computing techniques. It is not a single technique or a tool, rather it has become a complete subject, which involves various tools, techniques and frameworks. Structured data - Relational data. Semi Structured data - XML data. Unstructured data - Word, PDF, Text, Media Logs.","title":"BigData"},{"location":"hadoop/#properties","text":"Volume huge volume of data in terabytes Velocity Variety Veracity Value","title":"Properties"},{"location":"hadoop/#hadoop_1","text":"Apache Hadoop is a collection of open-source software utilities that facilitates using a network of many computers to solve problems involving massive amounts of data and computation. At its core, Hadoop has two major layers namely Processing/Computation layer (MapReduce), and Storage layer (Hadoop Distributed File System).","title":"Hadoop"},{"location":"hadoop/#ecosystem","text":"Hadoop Common contains libraries and utilities needed by the other hadoop modules Hadoop Distributed file system (HDFS) distributed file system that stores data on commodity hardware machines, providing very high aggregates bandwidth across the cluster Hadoop YARN platform responsible for managing computing resources in clusters and using them for scheduling users applications Hadoop MapReduce an implementation of the MapReduce programming model for large scale data processing Hadoop Ozone an object store for hadoop Apache Cassandra NoSQL DBMS constructed to manage huge volumes of data spread across numerous commodity servers, delivering high availability.","title":"Ecosystem"},{"location":"hadoop/#mapreduce","text":"MapReduce is a parallel programming model for writing distributed applications devised at Google for efficient processing of large amounts of data (multi-terabyte data-sets), on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner. The MapReduce program runs on Hadoop which is an Apache open-source framework.","title":"MapReduce"},{"location":"hadoop/#hdfs-hadoop-distributed-file-system","text":"NameNode master daemon maintains and manages datanode records metadata, e.g. location of block stored, the size of the files, permissions, hierarchy, etc. receives heartbeat and block report form all the DataNode DataNode Slave deamons stores actual data serves read and write requests The Hadoop Distributed File System (HDFS) is based on the Google File System (GFS) and provides a distributed file system that is designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. It is highly fault-tolerant and is designed to be deployed on low-cost hardware. It provides high throughput access to application data and is suitable for applications having large datasets.","title":"HDFS - Hadoop Distributed File System"},{"location":"hadoop/#hadoop-common","text":"These are Java libraries and utilities required by other Hadoop modules.","title":"Hadoop Common"},{"location":"hadoop/#hadoop-yarn","text":"This is a framework for job scheduling and cluster resource management. resource manager node manager","title":"Hadoop YARN"},{"location":"hadoop/mapreduce/","text":"MapReduce Examples WordCount EvenOdd MyCricket Box Classes Serialization is the process of converting object data into byte stream data for transmission over a network across different nodes in a cluster or for persistent data storage. Since hadoop use serialization for optimization, native java wrapper class are not used, but rather box classes similar to them are used in MapReduce programs. Java Native MapReduce Import Boolean BooleanWritable import org.apache.hadoop.io.BooleanWritable Byte ByteWritable import org.apache.hadoop.io.ByteWritable Integer IntWritable import org.apache.hadoop.io.IntWritable; long int VIntWritable import org.apache.hadoop.io.VIntWritable Float FloatWritable import org.apache.hadoop.io.FloatWritable Long LongWritable import org.apache.hadoop.io.LongWritable; long long VLongWritable import org.apache.hadoop.io.VLongWritable Double DoubleWritable import org.apache.hadoop.io.DoubleWritable; String Text import org.apache.hadoop.io.Text; Mapper Class Any mapper class for a MapReduce program extends the abstract Mapper class. And then we have to override the map function, which takes the key-value pair and reference to a Context variable, which is them handled by the reduce function. Basic template for a mapper class - public class [ mapper - class ] extends Mapper <[ key - type1 ] , [ value - type - 1 ] , [ key - type - 2 ] , [ value - type - 2 ]> { public void map ( [ key - type - 1 ] key , [ value - type - 1 ] value , Context context ) { // body of mapper } } Needed imports import java.io.IOException ; import org.apache.hadoop.mapreduce.Mapper ; import org.apache.hadoop.mapreduce.Mapper.Context ; Reducer Class Reducer class for a MapReduce program extends the abstract class Reducer . The reduce method is to be overridden in this class. Basic template of reducer class - public class [ reducer - class ] extends Reducer <[ key - type - 1 ] , [ value - type - 1 ] , [ key - type - 2 ] , [ value - type - 2 ]> { public void reduce ( [ key - type - 1 ] key , Iterable <[ value - type - 1 ]> values , Context context ){ //body of reducer } } Needed imports import java.io.IOException ; import org.apache.hadoop.mapreduce.Reducer ; import org.apache.hadoop.mapreduce.Reducer.Context ; Driver Class Driver class is the main class which controls the execution of the program. Here we create a Job object and set the driver, mapper, and reducer class used in our program. Basic template for a driver class- public class [ driver - class ] { public static void main ( String [] args ) { Job j = new Job (); j . setJobName ( \"My First Job\" ); j . setJarByClass ( [ driver - class ]. class ); j . setMapperClass ( [ mapper - class ]. class ); j . setReducerClass ( [ reducer - class ]. class ); j . setOutputKeyClass ( [ key - type ] . class ); j . setOutputValueClass ( [ value - type ] . class ); FileInputFormat . addInputPath ( j , new Path ( args [ 0 ] )); FileOutputFormat . setOutputPath ( j , new Path ( args [ 1 ] )); System . exit ( job . waitForCompletion ( true ) ? 0 : 1 ); } } Needed imports import org.apache.hadoop.mapreduce.Job ; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat ; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat ; import org.apache.hadoop.fs.FileSystem ; import org.apache.hadoop.fs.Path ; import java.io.IOException ; import org.apache.hadoop.conf.Configuration ; How to execute MapReduce Step 1 execute the following command to store the location of all the jar files needed for the execution of the map reduce program. H_CLASSPATH = $( hadoop classpath ) Check if the command was succesful. echo $H_CLASSPATH Step 2 compile to .java files to there respective .class files, either individually or all at once. For individual files, here -cp flag stands for classpath javac -cp $H_CLASSPATH filename.java or all .java files at once using wildcards like *.java javac -cp $H_CLASSPATH *.java Step 3 make a jar file of all the .class files, using this command jar -cvf [jarfilename.jar] *.class replace [jarfilename.jar] with the name of jar file you want. Step 4 put the files which you want to use MapReduce program on, to the HDFS Filesystem hadoop fs -put [ path to files ] Step 5 execute the MapReduce program hadoop jar [ jar-file-name.jar ] [ driver-class-name ] [ input-file ] [ output-folder ] Step 6 see the output hadoop fs -ls [output-folder] this will show the contents of the output folder, to see the contents of the files in the output folder use this command hadoop fs -cat [ output-folder ] / [ filename ] Example for WordCount program (clone this repo to try it, make sure you are in the WordCount directory while executing these commands) H_CLASSPATH = $( hadoop classpath ) javac *.java -cp $H_CLASSPATH jar -cvf wordcount.jar *.class hadoop fs -put poem.txt hadoop jar wordcount.jar WordCountDriver poem.txt wordcountout hadoop fs -ls wordcountout MapReduce (WordCount Example) Consider the MapReduce program for the WordCount program. Here are some things to consider first- Input file - poem.text Here is some part of the input file poem.txt What I won't tell you is how I became a flute and brushed against lips but there was no music. When the blows came furious as juniper. So input-value type is Text , and now we have to decide for the key value, since key value has to be unique we can choose the line no to be its key, and so input-key type is LongWritable . A example of key-value pair in this case is <0, \"What I won't tell you is how I became a flute\"> . In output of this program we want the total occurences of a particular word so output key-value pair will look like <\"the\", 5> . So here is summary of what will happen:- input file poem.txt driver class will recieve this input file driver class will then call the mapper function map function will receive key-value pairs in form { <0, \"What I won't tell you is how I became a flute\">, <1, \"and brushed against lips but there was no music.\">, ... } map function will generate key value pair of form { <\"What\", 1>, <\"I\", 1>, <\"won't\", 1>, <\"tell\", 1>, ...} reducer class will recieve the key-value pair generated by the mapper class output key-value pair generated by the reducer class { <\"What\", 13>, <\"I\", 2>, <\"won't\", 8>, <\"tell\", 100>, ...} output file part1000 Driver Class public class WordCountDriver { public static void main ( String [] args ) throws IOException , ClassNotFoundException , InterruptedException { Job j = new Job (); // creates a new job object. j . setJobName ( \"My First Job\" ); // sets the job name j . setJarByClass ( WordCountDriver . class ); // sets the name of the driver class j . setMapperClass ( WordCountMapper . class ); // sets the name of the mappper class j . setReducerClass ( WordCountReducer . class ); // sets the name of the reducer class j . setOutputKeyClass ( Text . class ); // sets the type of the output key j . setOutputValueClass ( IntWritable . class ); // sets the type of the output value FileInputFormat . addInputPath ( j , new Path ( args [ 0 ] )); // sets the input path, input file FileOutputFormat . setOutputPath ( j , new Path ( args [ 1 ] )); // sets the output path System . exit ( job . waitForCompletion ( true ) ? 0 : 1 ); // return the return value of the job execution } } Mapper Class Here are some thing we need to make a mapper class: input key type - LongWritable in this case input value type - Text in this case output key type - Text in this case output value type - IntWritable in this case public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable>{} the declaration of the mapper class Mapper<[key-type1], [value-type-1], [key-type-2], [value-type-2]> public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException{} overriding the map function public void map ( LongWritable key , Text value , Context context ) { /* sample input key - 0, value - \"What I won't tell you is how I became a flute\" key - 1, value - \"and brushed against lips but there was no music.\" */ /* convering the Text type to native String type so we can perform operations on it */ String inputstring = value . toString (); /* splitting the sentence into words */ /* \"What I won't tell you is how I became a flute\" will split into - \"What\", \"I\", \"won't\", ... */ for ( String x : inputstring . split ( \" \" )) { /* writing key value pairs to the context object */ context . write ( new Text ( x ), new IntWritable ( 1 )); } /* sample output, key value pairs written to the context object <\"What\", 1>, <\"I\", 1>, <\"won't\", 1>, ... */ } Reducer Class Here are some things we need to consider while making reducer class input key type - Text (same as mapper output key type) input value type - IntWritable (same as mapper output value type) output key type - Text in this case output value type - IntWritable in this case public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {} the declaration of the reducer class Reducer<[key-type-1], [value-type-1], [key-type-2], [value-type-2]> public void reduce(Text key, Iterable<IntWritable> values, Context context)throws IOException, InterruptedException {} overriding the reduce function /* suppose output of the reduce function is <\"the\", 1>, <\"a\", 1>, <\"the\", 1>, <\"hello\", 1>, <\"a\", 1>, <\"the\", 1> then reduce function will receive <\"the\", [1,1,1]>, <\"a\", [1,1]>, <\"hello\", [1]> i.e. a key and a iterable object contining the values and then the ouptut we need in this case is <\"the\", 3>, <\"a\", 2>, <\"hello\", 1> */ public void reduce ( Text key , Iterable < IntWritable > values , Context context ) int y = 0 ; /* looping over the values of the iterable object */ for ( IntWritable x : values ) { /* summing them up */ y ++ ; } /* writing the output key value pair to the contexet object */ context . write ( key , new IntWritable ( y )); } An then the final context object is passed back to the Job object in the driver class which then produces the output file. Execution H_CLASSPATH = $( hadoop classpath ) javac -cp $H_CLASSPATH *.java jar -cvf wordcount.jar *.class hadoop fs -put poem.txt hadoop jar wordcount.jar WordCountDriver poem.txt wordcountout hadoop fs -ls wordcountout hadoop fs -cat evenodd \\p art-r-00000","title":"MapReduce"},{"location":"hadoop/mapreduce/#mapreduce","text":"","title":"MapReduce"},{"location":"hadoop/mapreduce/#examples","text":"WordCount EvenOdd MyCricket","title":"Examples"},{"location":"hadoop/mapreduce/#box-classes","text":"Serialization is the process of converting object data into byte stream data for transmission over a network across different nodes in a cluster or for persistent data storage. Since hadoop use serialization for optimization, native java wrapper class are not used, but rather box classes similar to them are used in MapReduce programs. Java Native MapReduce Import Boolean BooleanWritable import org.apache.hadoop.io.BooleanWritable Byte ByteWritable import org.apache.hadoop.io.ByteWritable Integer IntWritable import org.apache.hadoop.io.IntWritable; long int VIntWritable import org.apache.hadoop.io.VIntWritable Float FloatWritable import org.apache.hadoop.io.FloatWritable Long LongWritable import org.apache.hadoop.io.LongWritable; long long VLongWritable import org.apache.hadoop.io.VLongWritable Double DoubleWritable import org.apache.hadoop.io.DoubleWritable; String Text import org.apache.hadoop.io.Text;","title":"Box Classes"},{"location":"hadoop/mapreduce/#mapper-class","text":"Any mapper class for a MapReduce program extends the abstract Mapper class. And then we have to override the map function, which takes the key-value pair and reference to a Context variable, which is them handled by the reduce function. Basic template for a mapper class - public class [ mapper - class ] extends Mapper <[ key - type1 ] , [ value - type - 1 ] , [ key - type - 2 ] , [ value - type - 2 ]> { public void map ( [ key - type - 1 ] key , [ value - type - 1 ] value , Context context ) { // body of mapper } } Needed imports import java.io.IOException ; import org.apache.hadoop.mapreduce.Mapper ; import org.apache.hadoop.mapreduce.Mapper.Context ;","title":"Mapper Class"},{"location":"hadoop/mapreduce/#reducer-class","text":"Reducer class for a MapReduce program extends the abstract class Reducer . The reduce method is to be overridden in this class. Basic template of reducer class - public class [ reducer - class ] extends Reducer <[ key - type - 1 ] , [ value - type - 1 ] , [ key - type - 2 ] , [ value - type - 2 ]> { public void reduce ( [ key - type - 1 ] key , Iterable <[ value - type - 1 ]> values , Context context ){ //body of reducer } } Needed imports import java.io.IOException ; import org.apache.hadoop.mapreduce.Reducer ; import org.apache.hadoop.mapreduce.Reducer.Context ;","title":"Reducer Class"},{"location":"hadoop/mapreduce/#driver-class","text":"Driver class is the main class which controls the execution of the program. Here we create a Job object and set the driver, mapper, and reducer class used in our program. Basic template for a driver class- public class [ driver - class ] { public static void main ( String [] args ) { Job j = new Job (); j . setJobName ( \"My First Job\" ); j . setJarByClass ( [ driver - class ]. class ); j . setMapperClass ( [ mapper - class ]. class ); j . setReducerClass ( [ reducer - class ]. class ); j . setOutputKeyClass ( [ key - type ] . class ); j . setOutputValueClass ( [ value - type ] . class ); FileInputFormat . addInputPath ( j , new Path ( args [ 0 ] )); FileOutputFormat . setOutputPath ( j , new Path ( args [ 1 ] )); System . exit ( job . waitForCompletion ( true ) ? 0 : 1 ); } } Needed imports import org.apache.hadoop.mapreduce.Job ; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat ; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat ; import org.apache.hadoop.fs.FileSystem ; import org.apache.hadoop.fs.Path ; import java.io.IOException ; import org.apache.hadoop.conf.Configuration ;","title":"Driver Class"},{"location":"hadoop/mapreduce/#how-to-execute-mapreduce","text":"","title":"How to execute MapReduce"},{"location":"hadoop/mapreduce/#step-1","text":"execute the following command to store the location of all the jar files needed for the execution of the map reduce program. H_CLASSPATH = $( hadoop classpath ) Check if the command was succesful. echo $H_CLASSPATH","title":"Step 1"},{"location":"hadoop/mapreduce/#step-2","text":"compile to .java files to there respective .class files, either individually or all at once. For individual files, here -cp flag stands for classpath javac -cp $H_CLASSPATH filename.java or all .java files at once using wildcards like *.java javac -cp $H_CLASSPATH *.java","title":"Step 2"},{"location":"hadoop/mapreduce/#step-3","text":"make a jar file of all the .class files, using this command jar -cvf [jarfilename.jar] *.class replace [jarfilename.jar] with the name of jar file you want.","title":"Step 3"},{"location":"hadoop/mapreduce/#step-4","text":"put the files which you want to use MapReduce program on, to the HDFS Filesystem hadoop fs -put [ path to files ]","title":"Step 4"},{"location":"hadoop/mapreduce/#step-5","text":"execute the MapReduce program hadoop jar [ jar-file-name.jar ] [ driver-class-name ] [ input-file ] [ output-folder ]","title":"Step 5"},{"location":"hadoop/mapreduce/#step-6","text":"see the output hadoop fs -ls [output-folder] this will show the contents of the output folder, to see the contents of the files in the output folder use this command hadoop fs -cat [ output-folder ] / [ filename ]","title":"Step 6"},{"location":"hadoop/mapreduce/#example","text":"for WordCount program (clone this repo to try it, make sure you are in the WordCount directory while executing these commands) H_CLASSPATH = $( hadoop classpath ) javac *.java -cp $H_CLASSPATH jar -cvf wordcount.jar *.class hadoop fs -put poem.txt hadoop jar wordcount.jar WordCountDriver poem.txt wordcountout hadoop fs -ls wordcountout","title":"Example"},{"location":"hadoop/mapreduce/#mapreduce-wordcount-example","text":"Consider the MapReduce program for the WordCount program. Here are some things to consider first- Input file - poem.text Here is some part of the input file poem.txt What I won't tell you is how I became a flute and brushed against lips but there was no music. When the blows came furious as juniper. So input-value type is Text , and now we have to decide for the key value, since key value has to be unique we can choose the line no to be its key, and so input-key type is LongWritable . A example of key-value pair in this case is <0, \"What I won't tell you is how I became a flute\"> . In output of this program we want the total occurences of a particular word so output key-value pair will look like <\"the\", 5> . So here is summary of what will happen:- input file poem.txt driver class will recieve this input file driver class will then call the mapper function map function will receive key-value pairs in form { <0, \"What I won't tell you is how I became a flute\">, <1, \"and brushed against lips but there was no music.\">, ... } map function will generate key value pair of form { <\"What\", 1>, <\"I\", 1>, <\"won't\", 1>, <\"tell\", 1>, ...} reducer class will recieve the key-value pair generated by the mapper class output key-value pair generated by the reducer class { <\"What\", 13>, <\"I\", 2>, <\"won't\", 8>, <\"tell\", 100>, ...} output file part1000","title":"MapReduce (WordCount Example)"},{"location":"hadoop/mapreduce/#driver-class_1","text":"public class WordCountDriver { public static void main ( String [] args ) throws IOException , ClassNotFoundException , InterruptedException { Job j = new Job (); // creates a new job object. j . setJobName ( \"My First Job\" ); // sets the job name j . setJarByClass ( WordCountDriver . class ); // sets the name of the driver class j . setMapperClass ( WordCountMapper . class ); // sets the name of the mappper class j . setReducerClass ( WordCountReducer . class ); // sets the name of the reducer class j . setOutputKeyClass ( Text . class ); // sets the type of the output key j . setOutputValueClass ( IntWritable . class ); // sets the type of the output value FileInputFormat . addInputPath ( j , new Path ( args [ 0 ] )); // sets the input path, input file FileOutputFormat . setOutputPath ( j , new Path ( args [ 1 ] )); // sets the output path System . exit ( job . waitForCompletion ( true ) ? 0 : 1 ); // return the return value of the job execution } }","title":"Driver Class"},{"location":"hadoop/mapreduce/#mapper-class_1","text":"Here are some thing we need to make a mapper class: input key type - LongWritable in this case input value type - Text in this case output key type - Text in this case output value type - IntWritable in this case public class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable>{} the declaration of the mapper class Mapper<[key-type1], [value-type-1], [key-type-2], [value-type-2]> public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException{} overriding the map function public void map ( LongWritable key , Text value , Context context ) { /* sample input key - 0, value - \"What I won't tell you is how I became a flute\" key - 1, value - \"and brushed against lips but there was no music.\" */ /* convering the Text type to native String type so we can perform operations on it */ String inputstring = value . toString (); /* splitting the sentence into words */ /* \"What I won't tell you is how I became a flute\" will split into - \"What\", \"I\", \"won't\", ... */ for ( String x : inputstring . split ( \" \" )) { /* writing key value pairs to the context object */ context . write ( new Text ( x ), new IntWritable ( 1 )); } /* sample output, key value pairs written to the context object <\"What\", 1>, <\"I\", 1>, <\"won't\", 1>, ... */ }","title":"Mapper Class"},{"location":"hadoop/mapreduce/#reducer-class_1","text":"Here are some things we need to consider while making reducer class input key type - Text (same as mapper output key type) input value type - IntWritable (same as mapper output value type) output key type - Text in this case output value type - IntWritable in this case public class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {} the declaration of the reducer class Reducer<[key-type-1], [value-type-1], [key-type-2], [value-type-2]> public void reduce(Text key, Iterable<IntWritable> values, Context context)throws IOException, InterruptedException {} overriding the reduce function /* suppose output of the reduce function is <\"the\", 1>, <\"a\", 1>, <\"the\", 1>, <\"hello\", 1>, <\"a\", 1>, <\"the\", 1> then reduce function will receive <\"the\", [1,1,1]>, <\"a\", [1,1]>, <\"hello\", [1]> i.e. a key and a iterable object contining the values and then the ouptut we need in this case is <\"the\", 3>, <\"a\", 2>, <\"hello\", 1> */ public void reduce ( Text key , Iterable < IntWritable > values , Context context ) int y = 0 ; /* looping over the values of the iterable object */ for ( IntWritable x : values ) { /* summing them up */ y ++ ; } /* writing the output key value pair to the contexet object */ context . write ( key , new IntWritable ( y )); } An then the final context object is passed back to the Job object in the driver class which then produces the output file.","title":"Reducer Class"},{"location":"hadoop/mapreduce/#execution","text":"H_CLASSPATH = $( hadoop classpath ) javac -cp $H_CLASSPATH *.java jar -cvf wordcount.jar *.class hadoop fs -put poem.txt hadoop jar wordcount.jar WordCountDriver poem.txt wordcountout hadoop fs -ls wordcountout hadoop fs -cat evenodd \\p art-r-00000","title":"Execution"},{"location":"hadoop/mapreduce/examples/EvenOdd/","text":"EvenOdd Driver Class Mapper Class Reducer Class Input File How to execute H_CLASSPATH=$(hadoop classpath) creating a varible to store the path for the jar files needed for compiling javac *.java -cp $H_CLASSPATH compiling files to create .class files jar -cvf even_odd.jar *.class creating jar file hadoop fs -put evenodd.txt uploading the input file to HDFS hadoop jar even_odd.jar MyDriver evenodd.txt evenodd executing the map reduce program hadoop fs -ls evenodd listing the output files hadoop fs -cat evenodd\\part-r-00000 lising the contents of output file (only if step 5 was successful)","title":"EvenOdd"},{"location":"hadoop/mapreduce/examples/EvenOdd/#evenodd","text":"Driver Class Mapper Class Reducer Class Input File","title":"EvenOdd"},{"location":"hadoop/mapreduce/examples/EvenOdd/#how-to-execute","text":"H_CLASSPATH=$(hadoop classpath) creating a varible to store the path for the jar files needed for compiling javac *.java -cp $H_CLASSPATH compiling files to create .class files jar -cvf even_odd.jar *.class creating jar file hadoop fs -put evenodd.txt uploading the input file to HDFS hadoop jar even_odd.jar MyDriver evenodd.txt evenodd executing the map reduce program hadoop fs -ls evenodd listing the output files hadoop fs -cat evenodd\\part-r-00000 lising the contents of output file (only if step 5 was successful)","title":"How to execute"},{"location":"hadoop/mapreduce/examples/MyCricket/","text":"MyCricket Driver Class Mapper Class Reducer Class Input File Input file structure 101,Shikhar Dhawan,100,10,2 101 - nunber Shikhar Dhawan - name of player 100 - total score 10 - total six 2 - total boundaries How to execute to get these files use wget https://raw.githubusercontent.com/shivanshu-semwal/hadoop/master/src/mapreduce/src/MyCricket/get.sh chmod +x get.sh ./get.sh cd MyCricket H_CLASSPATH=$(hadoop classpath) creating a varible to store the path for the jar files needed for compiling javac *.java -cp $H_CLASSPATH compiling files to create .class files jar -cvf cricket.jar *.class creating jar file hadoop fs -put input_cric.txt uploading the input file to HDFS hadoop jar cricket.jar MyCric input_cric.txt cricket_out executing the map reduce program hadoop fs -ls cricket_out listing the output files hadoop fs -cat cricket_out\\part-r-00000 lising the contents of output file (only if step 5 was successful)","title":"MyCricket"},{"location":"hadoop/mapreduce/examples/MyCricket/#mycricket","text":"Driver Class Mapper Class Reducer Class Input File Input file structure 101,Shikhar Dhawan,100,10,2 101 - nunber Shikhar Dhawan - name of player 100 - total score 10 - total six 2 - total boundaries","title":"MyCricket"},{"location":"hadoop/mapreduce/examples/MyCricket/#how-to-execute","text":"to get these files use wget https://raw.githubusercontent.com/shivanshu-semwal/hadoop/master/src/mapreduce/src/MyCricket/get.sh chmod +x get.sh ./get.sh cd MyCricket H_CLASSPATH=$(hadoop classpath) creating a varible to store the path for the jar files needed for compiling javac *.java -cp $H_CLASSPATH compiling files to create .class files jar -cvf cricket.jar *.class creating jar file hadoop fs -put input_cric.txt uploading the input file to HDFS hadoop jar cricket.jar MyCric input_cric.txt cricket_out executing the map reduce program hadoop fs -ls cricket_out listing the output files hadoop fs -cat cricket_out\\part-r-00000 lising the contents of output file (only if step 5 was successful)","title":"How to execute"},{"location":"hadoop/mapreduce/examples/WordCount/","text":"WordCount Driver Class Mapper Class Reducer Class Input File How to execute H_CLASSPATH=$(hadoop classpath) creating a varible to store the path for the jar files needed for compiling javac *.java -cp $H_CLASSPATH compiling files to create .class files jar -cvf wordcount.jar *.class creating jar file hadoop fs -put poem.txt uploading the input file to HDFS hadoop jar wordcount.jar WordCountDriver poem.txt wordcountout executing the map reduce program hadoop fs -ls wordcountout listing the output files hadoop fs -cat wordcountout\\part-r-00000 lising the contents of output file (only if step 5 was successful)","title":"WordCount"},{"location":"hadoop/mapreduce/examples/WordCount/#wordcount","text":"Driver Class Mapper Class Reducer Class Input File","title":"WordCount"},{"location":"hadoop/mapreduce/examples/WordCount/#how-to-execute","text":"H_CLASSPATH=$(hadoop classpath) creating a varible to store the path for the jar files needed for compiling javac *.java -cp $H_CLASSPATH compiling files to create .class files jar -cvf wordcount.jar *.class creating jar file hadoop fs -put poem.txt uploading the input file to HDFS hadoop jar wordcount.jar WordCountDriver poem.txt wordcountout executing the map reduce program hadoop fs -ls wordcountout listing the output files hadoop fs -cat wordcountout\\part-r-00000 lising the contents of output file (only if step 5 was successful)","title":"How to execute"},{"location":"julia/","text":"Julia Resources welcome - https://www.youtube.com/watch?v=AXgLWumAOhk data - https://www.youtube.com/watch?v=iG1dZBaxS-U https://github.com/JuliaAcademy/DataScience https://github.com/JuliaAcademy/JuliaTutorials https://github.com/Datseris/Zero2Hero-JuliaWorkshop","title":"Julia"},{"location":"julia/#julia","text":"","title":"Julia"},{"location":"julia/#resources","text":"welcome - https://www.youtube.com/watch?v=AXgLWumAOhk data - https://www.youtube.com/watch?v=iG1dZBaxS-U https://github.com/JuliaAcademy/DataScience https://github.com/JuliaAcademy/JuliaTutorials https://github.com/Datseris/Zero2Hero-JuliaWorkshop","title":"Resources"},{"location":"machine-learning/","text":"Machine Learning Resources https://www.youtube.com/channel/UC7p_I0qxYZP94vhesuLAWNA/videos https://www.youtube.com/channel/UCxsqJMTD-yOe277vtQIRjgw https://www.geeksforgeeks.org/machine-learning/ Where to get data https://archive.ics.uci.edu/ml/datasets.php https://data.gov.in/ https://www.kaggle.com/ Introduction Understanding and building methods that learn that is methods that leverage data to improve performance on some set of tasks. It is seen as a part of artificial intelligence. ML algorithms build a model based on sample data (training data) in order to make predictions or decisions without being explicitly programmed to do so. A subset of ML is closely related to computational statistics focuses on making predictions using computers but not all ML is statistical learning The study of mathematical optimization delivers methods, theory and application domain to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. Some implementations of ML use data and neural networks in a way that mimics the working of biological brain. In its application across business problems, ML is also referred to as predictive analysis . Outline Problems Classification Regression Clustering Dimension reduction density estimation Anomaly detection Data Cleaning AutoML Association rules Structured prediction Feature engineering Feature learning Online learning Reinforcement learning Supervised learning Semi-supervised learning Unsupervised learning Learning to rank Grammar induction Supervised learning (classification, regression) Decision tree Ensembles Bagging Boosting Random forest k-NN Linear Regression Naive Bayes Artificial Neural Networks Logistic regression Preceptron Relevance vector machine (RVM) Support vector machine (SVM) Clustering BIRCH CURE Hierarchial k-means Fuzzy Expectation-maximization (EM) DBSCAN OPTICS Mean shift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA PGD t-SNE Structured Prediction Graphical models Bayes net Conditional random field Hidden Markov Anomaly detection k-NN Local outlier factor Isolation forest Artificial neural network Autoencoder Cognitive computing Deep learning DeepDream Multilayer perceptron RNN LSTM GRU ESN reservoir computing Restricted Boltzmann machine GAN SOM Convolutional neural network (U-Net) Transformer (Vision) Spiking neural network Memtransistor Electrochemical RAM (ECRAM) Reinforcement learning Learning with humans Model diagnostics Theory Kernel machines Bias-variance tradeoff Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Machine learning venues Classification Supervised Learning Deep Neural Networks Dimensionality Reduction Unsupervised Learning Probabilistic Graphic Methods Sequential Learning Causal Inference Reinforcement Learning On basis of the nature of the learning signal or feedback available to a learning system Supervised learning Unsupervised learning Semi-supervised learning Reinforcement learning On the basis of output desired from a machine learned system Classification Regression Clustering Density estimation Dimensionality reduction Terminologies of Machine Learning Model Feature Target (Label) Training Prediction Libraries pandas first you want to load the data, for that you require pandas it converts the csv file you have into data frames matplotlib , seaborn , plotly these are for visualizing the data you have numpy when you want to convert your data into more compact usable for like numpy array which is faster than the normal list of python scipy when you want to perform some scientific calculations on the data like calculating the ode, fft, integration and differentiation scikit-learn this library contains list of algorithms used in machine learning like classification, regression, etc tensorflow if you want to make some neural networks, then you will need tensorflow keras its api to interact with tensorflow, you need to use keras, which is module inside tensorflow. Theses are basically all the libraries you will need for machine learning. PyTorch is an open source machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Meta AI. pip install pytorch TensorFlow is a free and open-source software library for machine learning and artificial intelligence. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks pip install tensorflow OpenCV is a library of programming functions mainly aimed at real-time computer vision. pip install opencv-python NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. pip install numpy SciPy is a free and open-source Python library used for scientific computing and technical computing. SciPy contains modules for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and other tasks common in science and engineering. pip install scipy Keras is an open-source software library that provides a Python interface for artificial neural networks. Keras acts as an interface for the TensorFlow library. Up until version 2.3, Keras supported multiple backends, including TensorFlow, Microsoft Cognitive Toolkit, Theano, and PlaidML Scikit-learn is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK. pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series.","title":"Machine Learning"},{"location":"machine-learning/#machine-learning","text":"","title":"Machine Learning"},{"location":"machine-learning/#resources","text":"https://www.youtube.com/channel/UC7p_I0qxYZP94vhesuLAWNA/videos https://www.youtube.com/channel/UCxsqJMTD-yOe277vtQIRjgw https://www.geeksforgeeks.org/machine-learning/","title":"Resources"},{"location":"machine-learning/#where-to-get-data","text":"https://archive.ics.uci.edu/ml/datasets.php https://data.gov.in/ https://www.kaggle.com/","title":"Where to get data"},{"location":"machine-learning/#introduction","text":"Understanding and building methods that learn that is methods that leverage data to improve performance on some set of tasks. It is seen as a part of artificial intelligence. ML algorithms build a model based on sample data (training data) in order to make predictions or decisions without being explicitly programmed to do so. A subset of ML is closely related to computational statistics focuses on making predictions using computers but not all ML is statistical learning The study of mathematical optimization delivers methods, theory and application domain to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. Some implementations of ML use data and neural networks in a way that mimics the working of biological brain. In its application across business problems, ML is also referred to as predictive analysis .","title":"Introduction"},{"location":"machine-learning/#outline","text":"Problems Classification Regression Clustering Dimension reduction density estimation Anomaly detection Data Cleaning AutoML Association rules Structured prediction Feature engineering Feature learning Online learning Reinforcement learning Supervised learning Semi-supervised learning Unsupervised learning Learning to rank Grammar induction Supervised learning (classification, regression) Decision tree Ensembles Bagging Boosting Random forest k-NN Linear Regression Naive Bayes Artificial Neural Networks Logistic regression Preceptron Relevance vector machine (RVM) Support vector machine (SVM) Clustering BIRCH CURE Hierarchial k-means Fuzzy Expectation-maximization (EM) DBSCAN OPTICS Mean shift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA PGD t-SNE Structured Prediction Graphical models Bayes net Conditional random field Hidden Markov Anomaly detection k-NN Local outlier factor Isolation forest Artificial neural network Autoencoder Cognitive computing Deep learning DeepDream Multilayer perceptron RNN LSTM GRU ESN reservoir computing Restricted Boltzmann machine GAN SOM Convolutional neural network (U-Net) Transformer (Vision) Spiking neural network Memtransistor Electrochemical RAM (ECRAM) Reinforcement learning Learning with humans Model diagnostics Theory Kernel machines Bias-variance tradeoff Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Machine learning venues","title":"Outline"},{"location":"machine-learning/#classification","text":"Supervised Learning Deep Neural Networks Dimensionality Reduction Unsupervised Learning Probabilistic Graphic Methods Sequential Learning Causal Inference Reinforcement Learning On basis of the nature of the learning signal or feedback available to a learning system Supervised learning Unsupervised learning Semi-supervised learning Reinforcement learning On the basis of output desired from a machine learned system Classification Regression Clustering Density estimation Dimensionality reduction Terminologies of Machine Learning Model Feature Target (Label) Training Prediction","title":"Classification"},{"location":"machine-learning/#libraries","text":"pandas first you want to load the data, for that you require pandas it converts the csv file you have into data frames matplotlib , seaborn , plotly these are for visualizing the data you have numpy when you want to convert your data into more compact usable for like numpy array which is faster than the normal list of python scipy when you want to perform some scientific calculations on the data like calculating the ode, fft, integration and differentiation scikit-learn this library contains list of algorithms used in machine learning like classification, regression, etc tensorflow if you want to make some neural networks, then you will need tensorflow keras its api to interact with tensorflow, you need to use keras, which is module inside tensorflow. Theses are basically all the libraries you will need for machine learning. PyTorch is an open source machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Meta AI. pip install pytorch TensorFlow is a free and open-source software library for machine learning and artificial intelligence. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks pip install tensorflow OpenCV is a library of programming functions mainly aimed at real-time computer vision. pip install opencv-python NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. pip install numpy SciPy is a free and open-source Python library used for scientific computing and technical computing. SciPy contains modules for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and other tasks common in science and engineering. pip install scipy Keras is an open-source software library that provides a Python interface for artificial neural networks. Keras acts as an interface for the TensorFlow library. Up until version 2.3, Keras supported multiple backends, including TensorFlow, Microsoft Cognitive Toolkit, Theano, and PlaidML Scikit-learn is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK. pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series.","title":"Libraries"},{"location":"machine-learning/notation/","text":"Notation returns the max value of \\(x\\) for which \\(f(x)\\) is maximum $$ x_x(f(x)) $$ returns the max value of \\(F\\) for which \\(g(F(x))\\) is maximum. $$ \\argmax_F(g(F)) = \\argmax_F(g(F(x))) $$","title":"Notation"},{"location":"machine-learning/notation/#notation","text":"returns the max value of \\(x\\) for which \\(f(x)\\) is maximum $$ x_x(f(x)) $$ returns the max value of \\(F\\) for which \\(g(F(x))\\) is maximum. $$ \\argmax_F(g(F)) = \\argmax_F(g(F(x))) $$","title":"Notation"},{"location":"machine-learning/supervised-learning/","text":"Supervised Learning dataset - labelled, \\(x_i\\) data, \\(y_i\\) corresponding label notation \\(N\\) data points, e.g. a person dataset $$ {(x_i, y_i)}^N_{i=1} $$ \\(x_i\\) - ith dataset, e.g. person \\(x_i^{(j)}\\) - ith dataset, jth feature, e.g. a person height \\(x_i^{(1)}\\) - first feature \\(x_i^{(2)}\\) - second feature \\(y_i\\) - label, target variable can be either an element belonging to a finite set of classes \\(\\{1,2,\\dots, c\\}\\) or a real number or a more complex structure like vector example for a email message problem \\(\\{\\text{spam}, \\text{not-spam}\\}\\) supervised learning using dataset to produce a model that takes feature \\(x\\) as input and output information that allows deducing the label of the feature vector. Training Historical data - \\(D = \\{(\\textbf{X}_1, Y_1), (\\textbf{X}_2, Y_2), \\dots (\\textbf{X}_n, Y_n)\\}\\) passed to Learning Algorithm which outputs a model \\(F\\) Prediction Input data \\(\\textbf{X} = \\{\\text{URL}, \\text{Title - Body}, \\text{Hyperlink} \\}\\) Model - \\(F(\\textbf{X}\\) ) Target Label \\(Y\\) for ecommerce site. Identify problem, collection of data, and extract features Model the mapping form input to output variables. Types of supervision Classification \\(Y\\) is categorical e.g. web page classification for a search engine, product classification into categories model \\(F\\) logistic regression, decision trees, random forest, SVM, naive bayes Regression \\(Y\\) is numeric e.g. - base price markup prediction for a product, forecasting demand for a product model \\(F\\) linear regression regression trees kernel regression Supervised Learning models Linear Models linear regression logistic regression SVM Tree based models decision tress random forest gradient boosting Neural Networks ANN CNN Other models k-nearest neighbors naive bayesian bayesian models Loss Function find good model select \\(F\\) minimizing loss function \\(L\\) on the training data \\(D\\) \\[ F^* = \\argmin_F\\left(\\sum_{i \\in D} L(Y_i, F(\\textbf{X}_i)\\right) \\] Possible loss functions squared loss $$ (y-F(\\textbf{X}))^2 $$ logistic loss $$ \\log(1+e^{-yF(\\textbf{X})}) $$ \\(y \\in \\{+1, -1\\}\\) used in logistic regression hinge loss $$ \\max(0, 1-yF(\\textbf{X})), y \\in {+1, -1} $$ \\(y \\in \\{+1, -1\\}\\) used in SVM Linear Models \\[ F(\\textbf{X}) = \\textbf{w} \\cdot \\textbf{X} \\] training learns weights \\(\\textbf{w}\\) that minimize loss \\[ F^* = \\argmin_F \\sum_{i \\in D} L(Y_i, \\textbf{w} \\cdot \\textbf{X}_i) \\] Prediction regression \\(Y = \\textbf{w} \\cdot \\textbf{X}\\) classification \\(\\textbf{w} \\cdot \\textbf{X} > \\text{threshold} \\implies Y = +1\\) \\(\\textbf{w} \\cdot \\textbf{X} < \\text{threshold} \\implies Y = -1\\) Overfitting Model fits training data well, low training error don't generalize well to the unseen data complex models with large numbers of parameters capture not only good patterns but also bad patterns of noise. Under fitting model lack the expressive powers, poor training error does not capture target distribution - poor test error simple linear distribution cannot capture target distribution Linear Models Regularization prevents overfitting by penalizing large weights \\[ F^* = \\argmin_F \\left( \\sum_{i \\in D} L( Y_i, F(\\textbf{X}_i \\cdot \\textbf{w}) + \\lambda \\Omega(\\textbf{w})) \\right) \\] \\(\\lambda\\) is the hyperparameter to control in \\(L_1\\) regularization \\(\\Omega(\\textbf{w}) = \\|\\textbf{w}\\|\\) \\(\\Omega(\\textbf{w}) = w_1 + w_2 + \\dots + w_n\\) sum of all weights of the values in \\(\\textbf{w}\\) Bias Variance Tradeoff Suppose we have a large dataset \\(D\\) We divide this dataset into smaller datasets, \\(D_1, D_2, \\dots, D_n\\) Now we train models on these datasets, \\(F_1, F_2, \\dots, F_n\\) Now we provide a input to these models, \\(\\textbf{X}\\) and get the predictions, \\(y_1, y_2, \\dots, y_n\\) , and the actual value of the prediction should be \\(y\\) Now, values \\(y_1, y_2, \\dots, y_n\\) can be close to each other - low variance in model far apart from each other - high variance in model values \\(y_1, y_2, \\dots, y_n\\) can be close to \\(y\\) - low bias in model can be far from \\(y\\) - high bias in model * low variance high variance low bias high bias","title":"Supervised Learning"},{"location":"machine-learning/supervised-learning/#supervised-learning","text":"dataset - labelled, \\(x_i\\) data, \\(y_i\\) corresponding label notation \\(N\\) data points, e.g. a person dataset $$ {(x_i, y_i)}^N_{i=1} $$ \\(x_i\\) - ith dataset, e.g. person \\(x_i^{(j)}\\) - ith dataset, jth feature, e.g. a person height \\(x_i^{(1)}\\) - first feature \\(x_i^{(2)}\\) - second feature \\(y_i\\) - label, target variable can be either an element belonging to a finite set of classes \\(\\{1,2,\\dots, c\\}\\) or a real number or a more complex structure like vector example for a email message problem \\(\\{\\text{spam}, \\text{not-spam}\\}\\) supervised learning using dataset to produce a model that takes feature \\(x\\) as input and output information that allows deducing the label of the feature vector.","title":"Supervised Learning"},{"location":"machine-learning/supervised-learning/#training","text":"Historical data - \\(D = \\{(\\textbf{X}_1, Y_1), (\\textbf{X}_2, Y_2), \\dots (\\textbf{X}_n, Y_n)\\}\\) passed to Learning Algorithm which outputs a model \\(F\\)","title":"Training"},{"location":"machine-learning/supervised-learning/#prediction","text":"Input data \\(\\textbf{X} = \\{\\text{URL}, \\text{Title - Body}, \\text{Hyperlink} \\}\\) Model - \\(F(\\textbf{X}\\) ) Target Label \\(Y\\) for ecommerce site. Identify problem, collection of data, and extract features Model the mapping form input to output variables.","title":"Prediction"},{"location":"machine-learning/supervised-learning/#types-of-supervision","text":"Classification \\(Y\\) is categorical e.g. web page classification for a search engine, product classification into categories model \\(F\\) logistic regression, decision trees, random forest, SVM, naive bayes Regression \\(Y\\) is numeric e.g. - base price markup prediction for a product, forecasting demand for a product model \\(F\\) linear regression regression trees kernel regression","title":"Types of supervision"},{"location":"machine-learning/supervised-learning/#supervised-learning-models","text":"Linear Models linear regression logistic regression SVM Tree based models decision tress random forest gradient boosting Neural Networks ANN CNN Other models k-nearest neighbors naive bayesian bayesian models","title":"Supervised Learning models"},{"location":"machine-learning/supervised-learning/#loss-function","text":"find good model select \\(F\\) minimizing loss function \\(L\\) on the training data \\(D\\) \\[ F^* = \\argmin_F\\left(\\sum_{i \\in D} L(Y_i, F(\\textbf{X}_i)\\right) \\]","title":"Loss Function"},{"location":"machine-learning/supervised-learning/#possible-loss-functions","text":"squared loss $$ (y-F(\\textbf{X}))^2 $$ logistic loss $$ \\log(1+e^{-yF(\\textbf{X})}) $$ \\(y \\in \\{+1, -1\\}\\) used in logistic regression hinge loss $$ \\max(0, 1-yF(\\textbf{X})), y \\in {+1, -1} $$ \\(y \\in \\{+1, -1\\}\\) used in SVM","title":"Possible loss functions"},{"location":"machine-learning/supervised-learning/#linear-models","text":"\\[ F(\\textbf{X}) = \\textbf{w} \\cdot \\textbf{X} \\] training learns weights \\(\\textbf{w}\\) that minimize loss \\[ F^* = \\argmin_F \\sum_{i \\in D} L(Y_i, \\textbf{w} \\cdot \\textbf{X}_i) \\]","title":"Linear Models"},{"location":"machine-learning/supervised-learning/#prediction_1","text":"regression \\(Y = \\textbf{w} \\cdot \\textbf{X}\\) classification \\(\\textbf{w} \\cdot \\textbf{X} > \\text{threshold} \\implies Y = +1\\) \\(\\textbf{w} \\cdot \\textbf{X} < \\text{threshold} \\implies Y = -1\\)","title":"Prediction"},{"location":"machine-learning/supervised-learning/#overfitting","text":"Model fits training data well, low training error don't generalize well to the unseen data complex models with large numbers of parameters capture not only good patterns but also bad patterns of noise.","title":"Overfitting"},{"location":"machine-learning/supervised-learning/#under-fitting","text":"model lack the expressive powers, poor training error does not capture target distribution - poor test error simple linear distribution cannot capture target distribution","title":"Under fitting"},{"location":"machine-learning/supervised-learning/#linear-models_1","text":"","title":"Linear Models"},{"location":"machine-learning/supervised-learning/#regularization","text":"prevents overfitting by penalizing large weights \\[ F^* = \\argmin_F \\left( \\sum_{i \\in D} L( Y_i, F(\\textbf{X}_i \\cdot \\textbf{w}) + \\lambda \\Omega(\\textbf{w})) \\right) \\] \\(\\lambda\\) is the hyperparameter to control in \\(L_1\\) regularization \\(\\Omega(\\textbf{w}) = \\|\\textbf{w}\\|\\) \\(\\Omega(\\textbf{w}) = w_1 + w_2 + \\dots + w_n\\) sum of all weights of the values in \\(\\textbf{w}\\)","title":"Regularization"},{"location":"machine-learning/supervised-learning/#bias-variance-tradeoff","text":"Suppose we have a large dataset \\(D\\) We divide this dataset into smaller datasets, \\(D_1, D_2, \\dots, D_n\\) Now we train models on these datasets, \\(F_1, F_2, \\dots, F_n\\) Now we provide a input to these models, \\(\\textbf{X}\\) and get the predictions, \\(y_1, y_2, \\dots, y_n\\) , and the actual value of the prediction should be \\(y\\) Now, values \\(y_1, y_2, \\dots, y_n\\) can be close to each other - low variance in model far apart from each other - high variance in model values \\(y_1, y_2, \\dots, y_n\\) can be close to \\(y\\) - low bias in model can be far from \\(y\\) - high bias in model * low variance high variance low bias high bias","title":"Bias Variance Tradeoff"},{"location":"machine-learning/unsupervised-learning/","text":"Unsupervised Learning dataset \\(\\{x_i\\}^N_{i=1}\\) unlabelled feature vectors only goal is to crete a algorithm that takes feature \\(x\\) as input and either transforms it into another vector or into a value that can be used to solve practical problems e.g. clustering - model returns the id of cluster dimensionality reduction - feature vector from the input \\(x\\) outlier detection - real no \\(v\\) that indicates how \\(x\\) is different from a typical example SVM - support vector machine hyperplane - defined by two parameter \\(\\textbf{x}\\) - input feature vector \\(\\textbf{w}\\) - a real valued vector \\(b\\) - a real number \\[ \\textbf{w} \\cdot \\textbf{x} - b = 0 \\] Decision boundary the boundary separating the examples of different classes is called decision boundary. \\[ y = \\operatorname{sign}(\\textbf{w} \\cdot \\textbf{x} - b) \\] the goal of the learning algorithm (SVM) is to leverage the dataset and find the optimal values of \\(\\textbf{w}^*\\) and \\(b^*\\) for \\(\\textbf{w}\\) and \\(b\\) these values are found using optimizing algorithms optimization problem \\(\\textbf{w} \\textbf{x}_i - b \\ge 1\\) , if \\(y_i = +1\\) \\(\\textbf{w} \\textbf{x}_i - b \\le -1\\) if \\(y_i = -1\\) finally the model is \\[ f(\\textbf{x}) = \\operatorname{sign}(\\textbf{w}^* - b^*) \\]","title":"Unsupervised Learning"},{"location":"machine-learning/unsupervised-learning/#unsupervised-learning","text":"dataset \\(\\{x_i\\}^N_{i=1}\\) unlabelled feature vectors only goal is to crete a algorithm that takes feature \\(x\\) as input and either transforms it into another vector or into a value that can be used to solve practical problems e.g. clustering - model returns the id of cluster dimensionality reduction - feature vector from the input \\(x\\) outlier detection - real no \\(v\\) that indicates how \\(x\\) is different from a typical example","title":"Unsupervised Learning"},{"location":"machine-learning/unsupervised-learning/#svm-support-vector-machine","text":"hyperplane - defined by two parameter \\(\\textbf{x}\\) - input feature vector \\(\\textbf{w}\\) - a real valued vector \\(b\\) - a real number \\[ \\textbf{w} \\cdot \\textbf{x} - b = 0 \\] Decision boundary the boundary separating the examples of different classes is called decision boundary. \\[ y = \\operatorname{sign}(\\textbf{w} \\cdot \\textbf{x} - b) \\] the goal of the learning algorithm (SVM) is to leverage the dataset and find the optimal values of \\(\\textbf{w}^*\\) and \\(b^*\\) for \\(\\textbf{w}\\) and \\(b\\) these values are found using optimizing algorithms optimization problem \\(\\textbf{w} \\textbf{x}_i - b \\ge 1\\) , if \\(y_i = +1\\) \\(\\textbf{w} \\textbf{x}_i - b \\le -1\\) if \\(y_i = -1\\) finally the model is \\[ f(\\textbf{x}) = \\operatorname{sign}(\\textbf{w}^* - b^*) \\]","title":"SVM - support vector machine"},{"location":"python/","text":"","title":"Index"},{"location":"r/","text":"","title":"Index"},{"location":"scala/","text":"Scala What is Scala ? programing language statistically typed this means, data a variable stores is defined before compiling general purpose can be used to make wide variety of applications is both functional object oriented aimed to address some criticism faced by java first appearance in 2004 fp vs oop fp use of functions where each function performs a specific task Fundamental elements used are variables and functions. The data in the functions are immutable. Importance is not given to data but to functions. It follows declarative programming model. It uses recursion for iteration. It is parallel programming supported. The statements does not need to follow a particular order while execution Does not have any access specifier. To add new data and functions is not so easy. No data hiding is possible. Hence, Security is not possible. oop Classes are used where instance of objects are created Fundamental elements used are objects and methods, The data used here are mutable data. Importance is given to data rather than procedures. It follows imperative programming model. It uses loops for iteration. It does not support parallel programming. The statements need to follow a order (bottom up approach while execution) Has three access specifiers namely, Public, Private and Protected. Provides and easy way to add new data and functions. Provides data hiding. Hence, secured programs are possible. How to use scala ? you should have jvm installed. download scala binary form the scala official website it comes with a repl read execute print loop some tools also help you set up scala for the system sbt - scala built tool is one of them it is a build system help to manage dependencies for the scala project basics hello world object HelloWorld extends App { println ( \"Hello, World!\" ) } singleton object, no need of class like in java compile it with scalac helloworld.scala run it with scala HelloWorld scala -e for interactive mode, and scripting var and val var for variables whose values will be changed - mutable val for variable whose value will not be changed - immutable objects object have states and behavior instance of a class syntax: object obj - name { } methods behavior def functionName ([ list of parameters ]) : [ return type ] = { //function body return [ expr ] } def printHello (){ } field val name = \"Shivanshi\" var age = 11 initialize variables var varname = values var varname : Type = value varname : Type = value conditionals object demo { def isEven ( s : Int ): Boolean = { if ( x % 2 == 0 ) true else false } } loops while loop is supported val i = 0 ; while ( i < 10 ){ println ( i ) i = i + 1 } for loop is supported but with some changes, and diff syntax for ( i <- 1 to 10 ){ println ( i ) } nested loops in one line for ( i <- 1 to 10 ; j <- 1 to 100 ) println ( i + ' ' + j ) same as for ( i <- 1 to 10 ){ for ( j <- 1 to 100 ) println ( i + ' ' + j ) } do while loop is not supported Data types in Scala all java data types are supported Byte , Short Int Long Float Double Char Boolean String and some additional types Unit - no value, equivalent of void in java ~ Null - null or empty reference AnyRef - a supertype of any reference ~ Nothing - a subtype of every other types Any - a supertype of any type for object Any -> ... -> Nothing for references AnyRef -> ... -> Null difference b/w null Null Nothing Unit Nil None null - literal, a value Null - a subtype of all reference types ~ Nothing It doesn\u2019t have any methods or values extends the Any type ~ Nil - empty list - List() ~ None - subtype of Option type, opposite of Some ~ Unit - void empty return type types of functions first order don't take functions as arguments higher order take functions as arguments nested functions Define function inside another function. def factorial ( x : Int ): Int = { // making a nested function def fact ( i : Int , acc : Int ): Int = { if ( i <= 1 ) acc else fact ( i - 1 , i * acc ) } fact ( x , 1 ) } anonymous Anonymous functions in source code are called function literals and at run time , function literals are instantiated into objects called function values Scala supports first-class functions which means functions can be expressed in function literal syntax, i.e., (x: Int) => x + 1 and that functions can be represented by objects which are called function values e.g. with one parameter var inc = (x:Int) => x+1 with two parameter var mul = (x: Int, y: Int) => x*y with zero parameter var userVal = () => { 345 } , println(userVal()) closures a function, whose return value depends on the value of one or more variables declared outside this function. variable declares outside the function is called - free variable variable in the definition is called bound variable e.g. val more = 10 // free variable var y = ( x : Int ) => x + more // x-> bound variable the function value (the object) that\u2019s created at runtime from this function literal is called a closure tail recursion recursion at the end use @tailrec annotation @tailrec def factorial ( x : Int , acc : Int ): Int = { if ( n <= 1 ) acc else factorial ( x - 1 , acc * i ) } input in scala var a = scala . io . StdIn . readInt () var b = scala . io . StdIn . readDouble () var c = scala . io . StdIn . readLine () try-catch exceptions try { doSomething () } catch { case ex : IOException => println ( \"Oops!\" ) case ex : NullPointerException => println ( \"Oops!!\" ) } finally { println ( \"this will execute every time even if code terminates in middle\" ) println ( \"so close files here\" ) } match val first = \"chips\" first match { case \"salt\" => println ( \"pepper\" ) case \"chips\" => println ( \"salsa\" ) case \"eggs\" => println ( \"bacon\" ) } def doChore ( chore : String ): String = chore match { case \"clean dishes\" => \"scrub, dry\" case \"cook dinner\" => \"chop, sizzle\" case _ => \"whine, complain\" } different types of for loops with single range for ( i <- 1 to 100 ){ println ( i ) } with multiple range for ( i <- 1 to 10 ; b <- 1 to 10 ){ println ( i + j ) } with collections for ( i <- List ( 2 , 3 , 5 , 6 )){ println ( i ) } with filters for ( i <- List ( 1 , 3 , 4 , 6 , 7 ) if i != 3 ; if i != 4 ) { println ( i ) } with yield var a = for ( i <- List ( 1 , 2 , 3 , 4 ) if i != 3 ; if i != 4 ) yield i intermediate classes class Point ( xc : Int , yc : Int ) { var x : Int = xc var y : Int = yc def move ( dx : Int , dy : Int ) { x = x + dx y = y + dy println ( \"Point x location : \" + x ); println ( \"Point y location : \" + y ); } } class Point ( val xc : Int , val yc : Int ) { var x : Int = xc var y : Int = yc def move ( dx : Int , dy : Int ) { x = x + dx y = y + dy println ( \"Point x location : \" + x ); println ( \"Point y location : \" + y ); } } class Location ( override val xc : Int , override val yc : Int , val zc : Int ) extends Point ( xc , yc ) { var z : Int = zc def move ( dx : Int , dy : Int , dz : Int ) { x = x + dx y = y + dy z = z + dz println ( \"Point x location : \" + x ); println ( \"Point y location : \" + y ); println ( \"Point z location : \" + z ); } } object Demo { def main ( args : Array [ String ]) { val loc = new Location ( 10 , 20 , 15 ); // Move to a new location loc . move ( 10 , 10 , 5 ); } } objects Singleton Objects A singleton is a class that can have only one instance, i.e., Object. You create singleton using the keyword object instead of class keyword. Since you can't instantiate a singleton object, you can't pass parameters to the primary constructor. e.g. class Point ( val xc : Int , val yc : Int ) { var x : Int = xc var y : Int = yc def move ( dx : Int , dy : Int ): Unit = { x = x + dx y = y + dy } } object Demo { def main ( args : Array [ String ]): Unit = { val point = new Point ( 10 , 20 ) printPoint () def printPoint { println ( \"Point x location : \" + point . x ); println ( \"Point y location : \" + point . y ); } } } here Demo is a singleton object. access modifiers private A private member is visible only inside the class or object that contains the member definition class Outer { class Inner { private def f (): Unit = { println ( \"f\" ) } class InnerMost { f () // OK } } new Inner . f () // Error: f is not accessible } protected A protected member is only accessible from subclasses of the class in which the member is defined public Unlike private and protected members, it is not required to specify Public keyword for Public members. There is no explicit modifier for public members. Such members can be accessed from anywhere. scope of protection Access modifiers in Scala can be augmented with qualifiers. A modifier of the form private[X] or protected[X] means that access is private or protected \"up to\" X , where X designates some enclosing package, class or singleton object. package society { package professional { class Executive { private [ professional ] var workDetails = null private [ society ] var friends = null private [ this ] var secrets = null def help ( another : Executive ) = { println ( another . workDetails ) println ( another . secrets ) //ERROR } } } } workDetails will be accessible to any class within the enclosing package professional. friends will be accessible to any class within the enclosing package society. secrets will be accessible only on the implicit object within instance methods (this). traits like interfaces in java encapsulate methods and field definition which can then be reused by mixing them into classes unlike inheritance, where only one class can be inherited no of traits can be mixed in one class traits can be partially implemented, have no constructors trait Equal { def isEqual ( x : Any ): Boolean def isNotEqual ( x : Any ): Boolean = ! isEqual ( x ) } using trait class Point ( xc : Int , yc : Int ) extends Equal { var x : Int = xc ; var y : Int = yc ; def isEqual ( obj : Any ) = { obj . isInstanceOf [ Point ] && obj . asInstanceOf [ Point ]. x == y } } object Demo { def main ( args : Array [ String ]): Unit = { val p1 = new Point ( 2 , 3 ) val p2 = new Point ( 2 , 4 ) val p3 = new Point ( 3 , 3 ) println ( p1 . isNotEqual ( p2 )) println ( p1 . isNotEqual ( p3 )) println ( p1 . isNotEqual ( 2 )) } } multiple traits, extends and then with trait Bark { def bark : String = \"Woof\" } trait Dog { def breed : String def color : String } class SaintBernard extends Dog with Bark { val breed = \"Saint Bernard\" val color = \"brown\" } Collections collection framework provides data structures for collecting one or more values of given types collection class hierarchy Iterable -> trait Collection ->trait Seq -> trait List -> sealed abstract Array -> final Array Buffer Set -> trait Set -> immutable Set -> mutable Map -> trait Map -> immutables Map -> mutable array contiguous memory allocated elements can be accesses using indices homogeneous elements val numbers = new Array [ Int ]( 5 ) numbers : Array [ Int ] = Array ( 0 , 0 , 0 , 0 , 0 ) arr.toList convert array to list arr.toString convert array to string array buffer dynamic memory allocation no need to specify the size // no need to specify the size val buf = new ArrayBuffer [ Int ]() // use some initial size val buf = new ArrayBuffer [ Int ]( 10 ) buf += 12 buf += 15 lists like arrays but immutable have recursive structure, where array don't homogeneous elements With type inference: var fruit = List ( \"apple\" , \"banana\" , \"pear\" ) var nums = List ( 1 , 2 , 3 , 4 , 5 ) var listinlist = List ( List ( 1 , 2 , 3 ), List ( 1 , 2 , 4 ), List ( 3 , 4 , 5 )) var emptyList = List () Without type inference: var fruit : List [ String ] = List ( \"apple\" , \"banana\" , \"pear\" ) var nums : List [ Int ] = List ( 1 , 2 , 3 , 4 , 5 ) var listinlist : List [ List [ Int ]] = List ( List ( 1 , 2 , 3 ), List ( 1 , 2 , 4 ), List ( 3 , 4 , 5 )) var emptyList : List [ Nothing ] = List () list types are covariant that means if S is subtype of T then List[S] is subtype of List[T] e.g. List[String] is subtype of List[Object] list is build form two fundamental building blocks Nil and :: (cons) Nil represent empty list :: (infix operator) expresses list extension ar the front x::xs - represents list with first element x followed by list xs val fruits = \"apple\" :: ( \"oranges\" :: ( \"bananan\" :: Nil )) basic operations on the list l.head returns the first element of the list l.tail returns a list with the first element removed l.isEmpty return true if list has no elements otherwise false l.length return the length of the list l.last - return the end element l.init - return a list removing the last element l.reverse - reverse the list l.toArray - convert list to array l.toString - convert list to a string l.copyTo(arr, startidx) - copy the list to arr starting from startidx of array l.take(n) - return a list of first n elements l.drop(n) - return a list of elements except first n elements l.splitAt(n) - return two list split at n l(n) return n element l.apply(n) return n` element l.indices return a list of the indices given in list l.mkString(prefix, separator, suffix) - set prefix, separator, and suffix while converting to string var l = List ( 'a' , 'b' , 'c' , 'd' , 'e' ) println ( l . mkString ( \"[\" , \",\" , \"]\" ) // [a,b,c,d,e] println ( l . mkString ( \"List(\" , \",\" , \"]\" ) // List(a,b,c,d,e] concatenating lists val a = List ( 1 , 2 ) ::: List ( 3 , 4 , 5 ) // or val a = List . concat ( List ( 1 , 2 ), List ( 3 , 4 , 5 )) pattern matching in list simple pattern matching val fruit = List ( \"apple\" , \"mango\" , \"banana\" ) val List ( a , b , c ) = fruit // a = \"apple\" // b = \"mango\" // c = \"banana\" when list size is unknown val fruit = List ( \"apple\" , \"mango\" , \"banana\" , \"pear\" , \"onion\" ) var a :: b :: rest = fruit // a = \"apple\" // b = \"mango\" // c = List(\"banana\", \"pear\", \"onion\") higher order function in list filter l - a list of type T fun - a predicate function of type T=>Boolean l.filter(fun) return a list of elements for which fun return true var a = List ( 1 , 2 , 3 , 4 , 5 ) var b = a . filter ( _ % 2 == 0 ) // b = List(2,4) partition l - a list of type T fun - a predicate function of type T=>Boolean l.partition(fun) return a tuple of list of elements for which fun return true and of elements for which fun return false find l - a list of type T fun - a predicate function of type T=>Boolean l.find(fun) - return first element satisfying a given predicate return None if no element is found takeWhile l - a list of type T fun - a predicate function of type T=>Boolean l.takeWhile(fun) - take values till fun return true dropWhile l - a list of type T fun - a predicate function of type T=>Boolean l.takeWhile(fun) - drop values till fun return true span l - a list of type T fun - a predicate function of type T=>Boolean l.span(fun) return a tuple (l.takeWhile(fun), l.dropWhile(fun)) map apply a function to whole list and return it val add10 : Int => Int = _ + 10 // A function taking an Int and returning an Int List ( 1 , 2 , 3 ) map add10 // List(11, 12, 13) - add10 is applied to each element foreach l - a list of type T fun - a predicate function of type T=>Unit l.foreach(fun) - run fun for every value in the list e.g. val aListOfNumbers = List ( 1 , 2 , 3 , 4 , 10 , 20 , 100 ) aListOfNumbers foreach ( x => println ( x )) aListOfNumbers foreach println tuples e.g. (1, 2) (4, 3, 2) (1, 2, \"three\") (a, 2, \"three\") val divideInts = (x: Int, y: Int) => (x / y, x % y) - function returning a tuple _._n - access n element of the tuple, 1 based index val d = divideInts ( 10 , 3 ) // (Int, Int) = (3,1) d . _1 // Int = 3 d . _2 // Int = 1 also use multiple variable assignment val ( div , mod ) = divideInts ( 10 , 3 ) div // Int = 3 mod // Int = 1 option Scala Option[T] is a container for zero or one element of a given type. An Option[T] can be either Some[T] or None object, which represents a missing value. ranges x to y by z = x until (y+1) by z val range = 0 until 10 give Range(0, 1, 2, 3, 4, 5, 6, 7, 8, 9) start = 0 end = 9 // not 10 but 9 step = 1 (0 to 10) by 5 gives Range(0, 5, 10) , start = 0 end = 10 step = 5 by is used to set the step size set unordered elements, implements using hashing cannot retrieve nth element as it is unordered ~ val colors = Set(\"red\", \"green\", \"blue\") adding new elements - colors + \"yellow\" removing elements - colors - \"green\" union elements - colors ++ Set(\"black\", \"white\") difference of set - colors -- Set(\"red\", \"green\") ~ s.head s.tail s.isEmpty s.min s.max s.intersect(s2) s.contains(ele) maps like look up tables stored as key, value val ordinals = Map(0 -> \"zero\", 1 -> \"one\", 2 -> \"two\") access element ordinals(2) gives \"two\" ~ s.keys return iterable containing keys s.values return iterable containing values s.isEmpty s.get(key) get value associated with key s.contains(key) check if key is present Data science data science process define business model------. <<--------------. map top ML problem | 80% work | data preparation | | exploratory data analysis--' | modelling---------------------------| 20% work | evaluation--------------------------'-----------' defining business model clearly define the problem set success criteria define clear data science objective map to ML model break business problems to data science problems identify the ML problem category data preparation understand the data and it's constraints formulate data analytics strategies perform required transformations EDA - exploratory data analysis perform statistical and visual analysis discover and handle outlier or errors shortlist predictive modelling techniques Modelling experiment with multiple models choose most optimal model create a feedback loop evaluation use model on real data test its accuracy if the model performs poorly start again form defining business model CRISP DM cross-industry standard for data mining .------------->>------>>----------->>-------------. | Business -------> Data | | .-->Understanding <--------- Understanding | | | | | | | +----+ | | ^ | |DATA| V V ^ | +----+ Data V ^ | Deployment Preparation | | | ^ | ^ | | | | | | | | | | V | | | '-----'--------Evaluation <------ Modelling | '-------<<--------<<-------------<<---------------' most widely used analytics model The outer circle in the diagram symbolizes the cyclic nature of data mining itself. A data mining process continues after a solution has been deployed. The lessons learned during the process can trigger new, often more focused business questions, and subsequent data mining processes will benefit from the experiences of previous ones.","title":"Scala"},{"location":"scala/#scala","text":"","title":"Scala"},{"location":"scala/#what-is-scala","text":"programing language statistically typed this means, data a variable stores is defined before compiling general purpose can be used to make wide variety of applications is both functional object oriented aimed to address some criticism faced by java first appearance in 2004","title":"What is Scala ?"},{"location":"scala/#fp-vs-oop","text":"","title":"fp vs oop"},{"location":"scala/#fp","text":"use of functions where each function performs a specific task Fundamental elements used are variables and functions. The data in the functions are immutable. Importance is not given to data but to functions. It follows declarative programming model. It uses recursion for iteration. It is parallel programming supported. The statements does not need to follow a particular order while execution Does not have any access specifier. To add new data and functions is not so easy. No data hiding is possible. Hence, Security is not possible.","title":"fp"},{"location":"scala/#oop","text":"Classes are used where instance of objects are created Fundamental elements used are objects and methods, The data used here are mutable data. Importance is given to data rather than procedures. It follows imperative programming model. It uses loops for iteration. It does not support parallel programming. The statements need to follow a order (bottom up approach while execution) Has three access specifiers namely, Public, Private and Protected. Provides and easy way to add new data and functions. Provides data hiding. Hence, secured programs are possible.","title":"oop"},{"location":"scala/#how-to-use-scala","text":"you should have jvm installed. download scala binary form the scala official website it comes with a repl read execute print loop some tools also help you set up scala for the system sbt - scala built tool is one of them it is a build system help to manage dependencies for the scala project","title":"How to use scala ?"},{"location":"scala/#basics","text":"","title":"basics"},{"location":"scala/#hello-world","text":"object HelloWorld extends App { println ( \"Hello, World!\" ) } singleton object, no need of class like in java compile it with scalac helloworld.scala run it with scala HelloWorld scala -e for interactive mode, and scripting","title":"hello world"},{"location":"scala/#var-and-val","text":"var for variables whose values will be changed - mutable val for variable whose value will not be changed - immutable","title":"var and val"},{"location":"scala/#objects","text":"object have states and behavior instance of a class syntax: object obj - name { }","title":"objects"},{"location":"scala/#methods","text":"behavior def functionName ([ list of parameters ]) : [ return type ] = { //function body return [ expr ] } def printHello (){ }","title":"methods"},{"location":"scala/#field","text":"val name = \"Shivanshi\" var age = 11","title":"field"},{"location":"scala/#initialize-variables","text":"var varname = values var varname : Type = value varname : Type = value","title":"initialize variables"},{"location":"scala/#conditionals","text":"object demo { def isEven ( s : Int ): Boolean = { if ( x % 2 == 0 ) true else false } }","title":"conditionals"},{"location":"scala/#loops","text":"while loop is supported val i = 0 ; while ( i < 10 ){ println ( i ) i = i + 1 } for loop is supported but with some changes, and diff syntax for ( i <- 1 to 10 ){ println ( i ) } nested loops in one line for ( i <- 1 to 10 ; j <- 1 to 100 ) println ( i + ' ' + j ) same as for ( i <- 1 to 10 ){ for ( j <- 1 to 100 ) println ( i + ' ' + j ) } do while loop is not supported","title":"loops"},{"location":"scala/#data-types-in-scala","text":"all java data types are supported Byte , Short Int Long Float Double Char Boolean String and some additional types Unit - no value, equivalent of void in java ~ Null - null or empty reference AnyRef - a supertype of any reference ~ Nothing - a subtype of every other types Any - a supertype of any type for object Any -> ... -> Nothing for references AnyRef -> ... -> Null","title":"Data types in Scala"},{"location":"scala/#difference-bw-null-null-nothing-unit-nil-none","text":"null - literal, a value Null - a subtype of all reference types ~ Nothing It doesn\u2019t have any methods or values extends the Any type ~ Nil - empty list - List() ~ None - subtype of Option type, opposite of Some ~ Unit - void empty return type","title":"difference b/w null Null Nothing Unit Nil None"},{"location":"scala/#types-of-functions","text":"","title":"types of functions"},{"location":"scala/#first-order","text":"don't take functions as arguments","title":"first order"},{"location":"scala/#higher-order","text":"take functions as arguments","title":"higher order"},{"location":"scala/#nested-functions","text":"Define function inside another function. def factorial ( x : Int ): Int = { // making a nested function def fact ( i : Int , acc : Int ): Int = { if ( i <= 1 ) acc else fact ( i - 1 , i * acc ) } fact ( x , 1 ) }","title":"nested functions"},{"location":"scala/#anonymous","text":"Anonymous functions in source code are called function literals and at run time , function literals are instantiated into objects called function values Scala supports first-class functions which means functions can be expressed in function literal syntax, i.e., (x: Int) => x + 1 and that functions can be represented by objects which are called function values e.g. with one parameter var inc = (x:Int) => x+1 with two parameter var mul = (x: Int, y: Int) => x*y with zero parameter var userVal = () => { 345 } , println(userVal())","title":"anonymous"},{"location":"scala/#closures","text":"a function, whose return value depends on the value of one or more variables declared outside this function. variable declares outside the function is called - free variable variable in the definition is called bound variable e.g. val more = 10 // free variable var y = ( x : Int ) => x + more // x-> bound variable the function value (the object) that\u2019s created at runtime from this function literal is called a closure","title":"closures"},{"location":"scala/#tail-recursion","text":"recursion at the end use @tailrec annotation @tailrec def factorial ( x : Int , acc : Int ): Int = { if ( n <= 1 ) acc else factorial ( x - 1 , acc * i ) }","title":"tail recursion"},{"location":"scala/#input-in-scala","text":"var a = scala . io . StdIn . readInt () var b = scala . io . StdIn . readDouble () var c = scala . io . StdIn . readLine ()","title":"input in scala"},{"location":"scala/#try-catch-exceptions","text":"try { doSomething () } catch { case ex : IOException => println ( \"Oops!\" ) case ex : NullPointerException => println ( \"Oops!!\" ) } finally { println ( \"this will execute every time even if code terminates in middle\" ) println ( \"so close files here\" ) }","title":"try-catch exceptions"},{"location":"scala/#match","text":"val first = \"chips\" first match { case \"salt\" => println ( \"pepper\" ) case \"chips\" => println ( \"salsa\" ) case \"eggs\" => println ( \"bacon\" ) } def doChore ( chore : String ): String = chore match { case \"clean dishes\" => \"scrub, dry\" case \"cook dinner\" => \"chop, sizzle\" case _ => \"whine, complain\" }","title":"match"},{"location":"scala/#different-types-of-for-loops","text":"with single range for ( i <- 1 to 100 ){ println ( i ) } with multiple range for ( i <- 1 to 10 ; b <- 1 to 10 ){ println ( i + j ) } with collections for ( i <- List ( 2 , 3 , 5 , 6 )){ println ( i ) } with filters for ( i <- List ( 1 , 3 , 4 , 6 , 7 ) if i != 3 ; if i != 4 ) { println ( i ) } with yield var a = for ( i <- List ( 1 , 2 , 3 , 4 ) if i != 3 ; if i != 4 ) yield i","title":"different types of for loops"},{"location":"scala/#intermediate","text":"","title":"intermediate"},{"location":"scala/#classes","text":"class Point ( xc : Int , yc : Int ) { var x : Int = xc var y : Int = yc def move ( dx : Int , dy : Int ) { x = x + dx y = y + dy println ( \"Point x location : \" + x ); println ( \"Point y location : \" + y ); } } class Point ( val xc : Int , val yc : Int ) { var x : Int = xc var y : Int = yc def move ( dx : Int , dy : Int ) { x = x + dx y = y + dy println ( \"Point x location : \" + x ); println ( \"Point y location : \" + y ); } } class Location ( override val xc : Int , override val yc : Int , val zc : Int ) extends Point ( xc , yc ) { var z : Int = zc def move ( dx : Int , dy : Int , dz : Int ) { x = x + dx y = y + dy z = z + dz println ( \"Point x location : \" + x ); println ( \"Point y location : \" + y ); println ( \"Point z location : \" + z ); } } object Demo { def main ( args : Array [ String ]) { val loc = new Location ( 10 , 20 , 15 ); // Move to a new location loc . move ( 10 , 10 , 5 ); } }","title":"classes"},{"location":"scala/#objects_1","text":"Singleton Objects A singleton is a class that can have only one instance, i.e., Object. You create singleton using the keyword object instead of class keyword. Since you can't instantiate a singleton object, you can't pass parameters to the primary constructor. e.g. class Point ( val xc : Int , val yc : Int ) { var x : Int = xc var y : Int = yc def move ( dx : Int , dy : Int ): Unit = { x = x + dx y = y + dy } } object Demo { def main ( args : Array [ String ]): Unit = { val point = new Point ( 10 , 20 ) printPoint () def printPoint { println ( \"Point x location : \" + point . x ); println ( \"Point y location : \" + point . y ); } } } here Demo is a singleton object.","title":"objects"},{"location":"scala/#access-modifiers","text":"","title":"access modifiers"},{"location":"scala/#private","text":"A private member is visible only inside the class or object that contains the member definition class Outer { class Inner { private def f (): Unit = { println ( \"f\" ) } class InnerMost { f () // OK } } new Inner . f () // Error: f is not accessible }","title":"private"},{"location":"scala/#protected","text":"A protected member is only accessible from subclasses of the class in which the member is defined","title":"protected"},{"location":"scala/#public","text":"Unlike private and protected members, it is not required to specify Public keyword for Public members. There is no explicit modifier for public members. Such members can be accessed from anywhere.","title":"public"},{"location":"scala/#scope-of-protection","text":"Access modifiers in Scala can be augmented with qualifiers. A modifier of the form private[X] or protected[X] means that access is private or protected \"up to\" X , where X designates some enclosing package, class or singleton object. package society { package professional { class Executive { private [ professional ] var workDetails = null private [ society ] var friends = null private [ this ] var secrets = null def help ( another : Executive ) = { println ( another . workDetails ) println ( another . secrets ) //ERROR } } } } workDetails will be accessible to any class within the enclosing package professional. friends will be accessible to any class within the enclosing package society. secrets will be accessible only on the implicit object within instance methods (this).","title":"scope of protection"},{"location":"scala/#traits","text":"like interfaces in java encapsulate methods and field definition which can then be reused by mixing them into classes unlike inheritance, where only one class can be inherited no of traits can be mixed in one class traits can be partially implemented, have no constructors trait Equal { def isEqual ( x : Any ): Boolean def isNotEqual ( x : Any ): Boolean = ! isEqual ( x ) } using trait class Point ( xc : Int , yc : Int ) extends Equal { var x : Int = xc ; var y : Int = yc ; def isEqual ( obj : Any ) = { obj . isInstanceOf [ Point ] && obj . asInstanceOf [ Point ]. x == y } } object Demo { def main ( args : Array [ String ]): Unit = { val p1 = new Point ( 2 , 3 ) val p2 = new Point ( 2 , 4 ) val p3 = new Point ( 3 , 3 ) println ( p1 . isNotEqual ( p2 )) println ( p1 . isNotEqual ( p3 )) println ( p1 . isNotEqual ( 2 )) } } multiple traits, extends and then with trait Bark { def bark : String = \"Woof\" } trait Dog { def breed : String def color : String } class SaintBernard extends Dog with Bark { val breed = \"Saint Bernard\" val color = \"brown\" }","title":"traits"},{"location":"scala/#collections","text":"collection framework provides data structures for collecting one or more values of given types","title":"Collections"},{"location":"scala/#collection-class-hierarchy","text":"Iterable -> trait Collection ->trait Seq -> trait List -> sealed abstract Array -> final Array Buffer Set -> trait Set -> immutable Set -> mutable Map -> trait Map -> immutables Map -> mutable","title":"collection class hierarchy"},{"location":"scala/#array","text":"contiguous memory allocated elements can be accesses using indices homogeneous elements val numbers = new Array [ Int ]( 5 ) numbers : Array [ Int ] = Array ( 0 , 0 , 0 , 0 , 0 ) arr.toList convert array to list arr.toString convert array to string","title":"array"},{"location":"scala/#array-buffer","text":"dynamic memory allocation no need to specify the size // no need to specify the size val buf = new ArrayBuffer [ Int ]() // use some initial size val buf = new ArrayBuffer [ Int ]( 10 ) buf += 12 buf += 15","title":"array buffer"},{"location":"scala/#lists","text":"like arrays but immutable have recursive structure, where array don't homogeneous elements With type inference: var fruit = List ( \"apple\" , \"banana\" , \"pear\" ) var nums = List ( 1 , 2 , 3 , 4 , 5 ) var listinlist = List ( List ( 1 , 2 , 3 ), List ( 1 , 2 , 4 ), List ( 3 , 4 , 5 )) var emptyList = List () Without type inference: var fruit : List [ String ] = List ( \"apple\" , \"banana\" , \"pear\" ) var nums : List [ Int ] = List ( 1 , 2 , 3 , 4 , 5 ) var listinlist : List [ List [ Int ]] = List ( List ( 1 , 2 , 3 ), List ( 1 , 2 , 4 ), List ( 3 , 4 , 5 )) var emptyList : List [ Nothing ] = List () list types are covariant that means if S is subtype of T then List[S] is subtype of List[T] e.g. List[String] is subtype of List[Object] list is build form two fundamental building blocks Nil and :: (cons) Nil represent empty list :: (infix operator) expresses list extension ar the front x::xs - represents list with first element x followed by list xs val fruits = \"apple\" :: ( \"oranges\" :: ( \"bananan\" :: Nil ))","title":"lists"},{"location":"scala/#basic-operations-on-the-list","text":"l.head returns the first element of the list l.tail returns a list with the first element removed l.isEmpty return true if list has no elements otherwise false l.length return the length of the list l.last - return the end element l.init - return a list removing the last element l.reverse - reverse the list l.toArray - convert list to array l.toString - convert list to a string l.copyTo(arr, startidx) - copy the list to arr starting from startidx of array l.take(n) - return a list of first n elements l.drop(n) - return a list of elements except first n elements l.splitAt(n) - return two list split at n l(n) return n element l.apply(n) return n` element l.indices return a list of the indices given in list l.mkString(prefix, separator, suffix) - set prefix, separator, and suffix while converting to string var l = List ( 'a' , 'b' , 'c' , 'd' , 'e' ) println ( l . mkString ( \"[\" , \",\" , \"]\" ) // [a,b,c,d,e] println ( l . mkString ( \"List(\" , \",\" , \"]\" ) // List(a,b,c,d,e]","title":"basic operations on the list"},{"location":"scala/#concatenating-lists","text":"val a = List ( 1 , 2 ) ::: List ( 3 , 4 , 5 ) // or val a = List . concat ( List ( 1 , 2 ), List ( 3 , 4 , 5 ))","title":"concatenating lists"},{"location":"scala/#pattern-matching-in-list","text":"simple pattern matching val fruit = List ( \"apple\" , \"mango\" , \"banana\" ) val List ( a , b , c ) = fruit // a = \"apple\" // b = \"mango\" // c = \"banana\" when list size is unknown val fruit = List ( \"apple\" , \"mango\" , \"banana\" , \"pear\" , \"onion\" ) var a :: b :: rest = fruit // a = \"apple\" // b = \"mango\" // c = List(\"banana\", \"pear\", \"onion\")","title":"pattern matching in list"},{"location":"scala/#higher-order-function-in-list","text":"","title":"higher order function in list"},{"location":"scala/#filter","text":"l - a list of type T fun - a predicate function of type T=>Boolean l.filter(fun) return a list of elements for which fun return true var a = List ( 1 , 2 , 3 , 4 , 5 ) var b = a . filter ( _ % 2 == 0 ) // b = List(2,4)","title":"filter"},{"location":"scala/#partition","text":"l - a list of type T fun - a predicate function of type T=>Boolean l.partition(fun) return a tuple of list of elements for which fun return true and of elements for which fun return false","title":"partition"},{"location":"scala/#find","text":"l - a list of type T fun - a predicate function of type T=>Boolean l.find(fun) - return first element satisfying a given predicate return None if no element is found","title":"find"},{"location":"scala/#takewhile","text":"l - a list of type T fun - a predicate function of type T=>Boolean l.takeWhile(fun) - take values till fun return true","title":"takeWhile"},{"location":"scala/#dropwhile","text":"l - a list of type T fun - a predicate function of type T=>Boolean l.takeWhile(fun) - drop values till fun return true","title":"dropWhile"},{"location":"scala/#span","text":"l - a list of type T fun - a predicate function of type T=>Boolean l.span(fun) return a tuple (l.takeWhile(fun), l.dropWhile(fun))","title":"span"},{"location":"scala/#map","text":"apply a function to whole list and return it val add10 : Int => Int = _ + 10 // A function taking an Int and returning an Int List ( 1 , 2 , 3 ) map add10 // List(11, 12, 13) - add10 is applied to each element","title":"map"},{"location":"scala/#foreach","text":"l - a list of type T fun - a predicate function of type T=>Unit l.foreach(fun) - run fun for every value in the list e.g. val aListOfNumbers = List ( 1 , 2 , 3 , 4 , 10 , 20 , 100 ) aListOfNumbers foreach ( x => println ( x )) aListOfNumbers foreach println","title":"foreach"},{"location":"scala/#tuples","text":"e.g. (1, 2) (4, 3, 2) (1, 2, \"three\") (a, 2, \"three\") val divideInts = (x: Int, y: Int) => (x / y, x % y) - function returning a tuple _._n - access n element of the tuple, 1 based index val d = divideInts ( 10 , 3 ) // (Int, Int) = (3,1) d . _1 // Int = 3 d . _2 // Int = 1 also use multiple variable assignment val ( div , mod ) = divideInts ( 10 , 3 ) div // Int = 3 mod // Int = 1","title":"tuples"},{"location":"scala/#option","text":"Scala Option[T] is a container for zero or one element of a given type. An Option[T] can be either Some[T] or None object, which represents a missing value.","title":"option"},{"location":"scala/#ranges","text":"x to y by z = x until (y+1) by z val range = 0 until 10 give Range(0, 1, 2, 3, 4, 5, 6, 7, 8, 9) start = 0 end = 9 // not 10 but 9 step = 1 (0 to 10) by 5 gives Range(0, 5, 10) , start = 0 end = 10 step = 5 by is used to set the step size","title":"ranges"},{"location":"scala/#set","text":"unordered elements, implements using hashing cannot retrieve nth element as it is unordered ~ val colors = Set(\"red\", \"green\", \"blue\") adding new elements - colors + \"yellow\" removing elements - colors - \"green\" union elements - colors ++ Set(\"black\", \"white\") difference of set - colors -- Set(\"red\", \"green\") ~ s.head s.tail s.isEmpty s.min s.max s.intersect(s2) s.contains(ele)","title":"set"},{"location":"scala/#maps","text":"like look up tables stored as key, value val ordinals = Map(0 -> \"zero\", 1 -> \"one\", 2 -> \"two\") access element ordinals(2) gives \"two\" ~ s.keys return iterable containing keys s.values return iterable containing values s.isEmpty s.get(key) get value associated with key s.contains(key) check if key is present","title":"maps"},{"location":"scala/#data-science","text":"","title":"Data science"},{"location":"scala/#data-science-process","text":"define business model------. <<--------------. map top ML problem | 80% work | data preparation | | exploratory data analysis--' | modelling---------------------------| 20% work | evaluation--------------------------'-----------' defining business model clearly define the problem set success criteria define clear data science objective map to ML model break business problems to data science problems identify the ML problem category data preparation understand the data and it's constraints formulate data analytics strategies perform required transformations EDA - exploratory data analysis perform statistical and visual analysis discover and handle outlier or errors shortlist predictive modelling techniques Modelling experiment with multiple models choose most optimal model create a feedback loop evaluation use model on real data test its accuracy if the model performs poorly start again form defining business model","title":"data science process"},{"location":"scala/#crisp-dm","text":"cross-industry standard for data mining .------------->>------>>----------->>-------------. | Business -------> Data | | .-->Understanding <--------- Understanding | | | | | | | +----+ | | ^ | |DATA| V V ^ | +----+ Data V ^ | Deployment Preparation | | | ^ | ^ | | | | | | | | | | V | | | '-----'--------Evaluation <------ Modelling | '-------<<--------<<-------------<<---------------' most widely used analytics model The outer circle in the diagram symbolizes the cyclic nature of data mining itself. A data mining process continues after a solution has been deployed. The lessons learned during the process can trigger new, often more focused business questions, and subsequent data mining processes will benefit from the experiences of previous ones.","title":"CRISP DM"},{"location":"spark/","text":"","title":"Index"}]}